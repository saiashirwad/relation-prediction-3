{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RELATION PREDICTION",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "166409IxFYs-1tK8RNhEc_aCB0dFfmx2l",
      "authorship_tag": "ABX9TyOaUimi00DOxDqJNk51belW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e8e659b731eb476caf34a9af0a866a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a2c995dd0be34d5483a35c070dca8184",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_080dec54dbf0417f93071e6c68ebfc5f",
              "IPY_MODEL_1569f175025543c0b0cd9a518ea68b5b"
            ]
          }
        },
        "a2c995dd0be34d5483a35c070dca8184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "080dec54dbf0417f93071e6c68ebfc5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_43639be9f06c4ec28ec4a080cda53c81",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 5000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e47c290c33340b08f0757f227a8bbab"
          }
        },
        "1569f175025543c0b0cd9a518ea68b5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_84d26dd1b66448d1aa309f2ac6d94717",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/5000 [00:05&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c06457741374abd8d81b89044a09560"
          }
        },
        "43639be9f06c4ec28ec4a080cda53c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e47c290c33340b08f0757f227a8bbab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "84d26dd1b66448d1aa309f2ac6d94717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c06457741374abd8d81b89044a09560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saiashirwad/relation-prediction-3/blob/master/RELATION_PREDICTION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oud8mDSq7wWB",
        "colab_type": "code",
        "outputId": "e22c7066-6915-4672-a0be-ec542dc7e181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git clone https://github.com/saiashirwad/relation-prediction-3.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'relation-prediction-3' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-RjdUmozMYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html --quiet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reX5e1At8ekp",
        "colab_type": "code",
        "outputId": "d70d93e0-9d19-4222-bbd5-d0ebe8e36fb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  2 07:11:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    31W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkOJPEfvkhvh",
        "colab_type": "code",
        "outputId": "57a8c722-36d6-4a5f-c88c-072b5ab9dcff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd relation-prediction-3/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/relation-prediction-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrrUGd0j8XpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %%\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F \n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_scatter import scatter \n",
        "\n",
        "import numpy as np \n",
        "\n",
        "import os \n",
        "\n",
        "from layers import * \n",
        "from loss import * \n",
        "from evaluation import * \n",
        "from utils import * \n",
        "from dataloader import * \n",
        "from rotate import *\n",
        "\n",
        "import IPython\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-o2SLIL8wl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KGLayer(nn.Module):\n",
        "    def __init__(self, n_entities, n_relations, in_dim, out_dim, input_drop=0.5, \n",
        "                 margin=6.0, epsilon=2.0, device=\"cuda\", concat=True):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_entities = n_entities\n",
        "        self.n_relations = n_relations\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.device = device\n",
        "\n",
        "        self.margin = margin \n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.a = nn.Linear(3 * in_dim, out_dim).to(device)\n",
        "        nn.init.xavier_normal_(self.a.weight.data, gain=1.414)\n",
        "\n",
        "        self.a_2 = nn.Linear(out_dim, 1).to(device)\n",
        "        nn.init.xavier_normal_(self.a_2.weight.data, gain=1.414)\n",
        "\n",
        "        self.sparse_neighborhood_aggregation = SparseNeighborhoodAggregation()\n",
        "\n",
        "        self.concat = concat \n",
        "\n",
        "        if concat:\n",
        "            self.ent_embed_range = nn.Parameter(\n",
        "                torch.Tensor([(self.margin + self.epsilon) / self.out_dim]), \n",
        "                requires_grad = False\n",
        "            )\n",
        "            \n",
        "            self.rel_embed_range = nn.Parameter(\n",
        "                torch.Tensor([(self.margin + self.epsilon) / self.out_dim]),\n",
        "                requires_grad = False\n",
        "            )\n",
        "    \n",
        "            self.ent_embed = nn.Embedding(n_entities, in_dim, max_norm=1, norm_type=2).to(device)\n",
        "            self.rel_embed = nn.Embedding(n_relations, in_dim, max_norm=1, norm_type=2).to(device)\n",
        "            \n",
        "            nn.init.uniform_(self.ent_embed.weight.data, -self.ent_embed_range.item(), self.ent_embed_range.item())\n",
        "            nn.init.uniform_(self.rel_embed.weight.data, -self.rel_embed_range.item(), self.rel_embed_range.item())\n",
        "\n",
        "        self.input_drop = nn.Dropout(input_drop)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(3 * in_dim).to(device)\n",
        "        self.bn1 = nn.BatchNorm1d(out_dim).to(device)\n",
        "    \n",
        "    def forward(self, triplets, ent_embed=None, rel_embed=None):\n",
        "        N = self.n_entities\n",
        "    \n",
        "        if self.concat:\n",
        "            h = torch.cat((\n",
        "                self.ent_embed(triplets[:, 0]),\n",
        "                self.rel_embed(triplets[:, 1]),\n",
        "                self.ent_embed(triplets[:, 2])\n",
        "            ), dim=1)\n",
        "        else:\n",
        "            h = torch.cat((\n",
        "                ent_embed[triplets[:, 0]],\n",
        "                rel_embed[triplets[:, 1]],\n",
        "                ent_embed[triplets[:, 2]]\n",
        "            ), dim=1)\n",
        "\n",
        "        h = self.input_drop(self.bn0(h))\n",
        "        c = self.bn1(self.a(h))\n",
        "        b = -F.leaky_relu(self.a_2(c))\n",
        "        e_b = torch.exp(b) \n",
        "\n",
        "        temp = triplets.t()\n",
        "        edges = torch.stack([temp[0], temp[2]])\n",
        "\n",
        "        ebs = self.sparse_neighborhood_aggregation(edges, e_b, N, e_b.shape[0], 1)\n",
        "        temp1 = e_b * c\n",
        "\n",
        "        hs = self.sparse_neighborhood_aggregation(edges, temp1,  N, e_b.shape[0], self.out_dim)\n",
        "\n",
        "        ebs[ebs == 0] = 1e-12\n",
        "        h_ent = hs / ebs \n",
        "\n",
        "        index = triplets[:, 1]\n",
        "        h_rel  = scatter(temp1, index=index, dim=0, reduce=\"mean\") \n",
        "\n",
        "        return h_ent, h_rel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcfw7ro3OdYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RotAttLayer(nn.Module):\n",
        "    def __init__(self, n_ent, n_rel, in_dim, out_dim, n_heads=1, input_drop=0.5, negative_rate = 10, margin=6.0, epsilon=2.0, batch_size=None, device=\"cuda\"):\n",
        "        super().__init__() \n",
        "\n",
        "        self.n_heads = n_heads \n",
        "        self.device = device\n",
        "\n",
        "        self.in_dim = in_dim \n",
        "        self.out_dim = out_dim \n",
        "        self.margin = margin\n",
        "        self.epsilon = epsilon\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.negative_rate = negative_rate \n",
        "\n",
        "        self.embedding_range = nn.Parameter(\n",
        "            torch.Tensor([(self.margin + self.epsilon) / in_dim]), \n",
        "            requires_grad=False\n",
        "        )    \n",
        "    def rotate(self, h, r, t, mode):\n",
        "        pi = 3.14159265358979323846\n",
        "\n",
        "        re_head, im_head = torch.chunk(h, 2, dim=-1)\n",
        "        re_tail, im_tail = torch.chunk(t, 2, dim=-1)\n",
        "\n",
        "        phase_relation = r / (self.embedding_range.item() / pi) \n",
        "        \n",
        "        re_relation = torch.cos(phase_relation)\n",
        "        im_relation = torch.sin(phase_relation)\n",
        "        \n",
        "        if mode == 'head-batch':\n",
        "            re_score = re_relation * re_tail + im_relation * im_tail\n",
        "            im_score = re_relation * im_tail - im_relation * re_tail\n",
        "            re_score = re_score - re_head\n",
        "            im_score = im_score - im_head\n",
        "        else:\n",
        "            re_score = re_head * re_relation - im_head * im_relation\n",
        "            im_score = re_head * im_relation + im_head * re_relation\n",
        "            re_score = re_score - re_tail\n",
        "            im_score = im_score - im_tail\n",
        "\n",
        "        score = torch.stack([re_score, im_score], dim = 0)\n",
        "        score = score.norm(dim = 0)\n",
        "\n",
        "        score = self.margin - score.sum(dim = 2)\n",
        "        return score\n",
        "\n",
        "    def forward(self, sample, ent_embed, rel_embed, mode=\"single\"):\n",
        "        if mode == 'single':\n",
        "            batch_size, negative_sample_size = sample.size(0), 1\n",
        "                        \n",
        "            head = torch.index_select(ent_embed, dim=0, index=sample[:,0]).unsqueeze(1)\n",
        "            relation = torch.index_select(rel_embed, dim=0, index=sample[:,1]).unsqueeze(1)\n",
        "            tail = torch.index_select(ent_embed, dim=0, index=sample[:,2]).unsqueeze(1)\n",
        "            \n",
        "        elif mode == 'head-batch':\n",
        "            tail_part, head_part = sample\n",
        "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
        "            \n",
        "            head = torch.index_select(ent_embed, dim=0, index=head_part.view(-1)).view(batch_size, negative_sample_size, -1)      \n",
        "            relation = torch.index_select(rel_embed, dim=0, index=tail_part[:, 1]).unsqueeze(1)\n",
        "            tail = torch.index_select(ent_embed, dim=0, index=tail_part[:, 2]).unsqueeze(1)\n",
        "            \n",
        "        elif mode == 'tail-batch':\n",
        "            head_part, tail_part = sample\n",
        "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
        "\n",
        "            head = torch.index_select(ent_embed, dim=0, index=head_part[:, 0]).unsqueeze(1)\n",
        "            relation = torch.index_select(rel_embed, dim=0, index=head_part[:, 1]).unsqueeze(1)\n",
        "            tail = torch.index_select(ent_embed, dim=0, index=tail_part.view(-1)).view(batch_size, negative_sample_size, -1)\n",
        "            \n",
        "        score = self.rotate(head, relation, tail, mode)\n",
        "        \n",
        "        return score\n",
        "\n",
        "class RotAtte(nn.Module):\n",
        "    def __init__(self, n_ent, n_rel, in_dim, out_dim, n_heads=1, input_drop=0.5, negative_rate = 10, margin=6.0, epsilon=2.0, batch_size=None, device=\"cuda\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_ent = n_ent \n",
        "        self.n_rel = n_rel \n",
        "        self.in_dim = in_dim \n",
        "        self.out_dim = out_dim\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.a = nn.ModuleList([\n",
        "            KGLayer(\n",
        "                n_ent, n_rel, in_dim, out_dim, input_drop, margin=margin, epsilon=epsilon\n",
        "            )\n",
        "        for _ in range(self.n_heads)])\n",
        "\n",
        "        self.rotate = RotAttLayer(n_ent, n_rel, in_dim, out_dim, n_heads=1, input_drop=0.5, negative_rate = negative_rate, margin=margin, epsilon=epsilon, batch_size=batch_size, device=device) \n",
        "\n",
        "        self.ent_transform = nn.Linear(n_heads * out_dim, out_dim).to(device)\n",
        "        self.rel_transform = nn.Linear(n_heads * out_dim, out_dim // 2).to(device)\n",
        "    \n",
        "    def forward(self, sample, triplets, mode=\"single\"):\n",
        "        out = [a(triplets) for a in self.a]\n",
        "        ent_embed = self.ent_transform(torch.cat([o[0] for o in out], dim=1))\n",
        "        rel_embed = self.rel_transform(torch.cat([o[1] for o in out], dim=1))\n",
        "\n",
        "        # mask_indices = torch.unique( torch.cat([ batch_triplets[:, 0], batch_triplets[:, 2]]) )\n",
        "        if mode == 'single':\n",
        "            mask_indices = torch.unique(torch.cat([ sample[:, 0], sample[:, 2] ]))\n",
        "        elif mode == 'tail-batch':\n",
        "            mask_indices = torch.unique(torch.cat([ sample[0][:, 0], sample[0][:, 2], sample[1].flatten()]))\n",
        "        elif mode == 'head-batch':\n",
        "            mask_indices = torch.unique(torch.cat([ sample[1][:, 0], sample[1][:, 2], sample[0].flatten()]))\n",
        "        mask = torch.zeros(self.n_ent).to(self.device)\n",
        "        mask[mask_indices] = 1.0\n",
        "        ent_embed = mask.unsqueeze(-1).expand_as(ent_embed) * ent_embed \n",
        "        score = self.rotate(sample, ent_embed, rel_embed, mode)\n",
        "\n",
        "        return score \n",
        "    \n",
        "    def regularization(self):\n",
        "        pass "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZCym8QWFEtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RotAttTrainer:\n",
        "    def __init__(self, name=\"lol\", model = None, dataset=\"FB15k-237\", n_epochs=1000, batch_size=2000, device=\"cuda\", \n",
        "        optim_ = \"sgd\", lr = 0.001, checkpoint_dir=\"checkpoints\"):\n",
        "        self.name = name\n",
        "\n",
        "        in_dim = 500\n",
        "        out_dim = 500\n",
        "        \n",
        "        self.work_threads = 4 \n",
        "        self.lr = lr \n",
        "        self.weight_decay = None\n",
        "        self.n_epochs = n_epochs\n",
        "        self.device = device\n",
        "        self.adversarial_temperature = 1.0\n",
        "        self.negative_sample_size = 10\n",
        "        self.batch_size = batch_size\n",
        "        self.test_batch_size = 100\n",
        "    \n",
        "        self._load_data(dataset)\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "\n",
        "        if model is None:\n",
        "            self.model = RotAtte(self.n_ent, self.n_rel, in_dim, out_dim, batch_size=batch_size)\n",
        "        else:\n",
        "            self.model = model\n",
        "\n",
        "        self.optimizer = optim.SGD(self.model.parameters(), lr)\n",
        "    \n",
        "    def save_model(self):\n",
        "        torch.save(self.model.state_dict(), f\"{self.checkpoint_dir}/{self.name}\")\n",
        "    \n",
        "    def load_model(self):\n",
        "        self.model.load_state_dict(torch.load(f\"{self.checkpoint_dir}/{self.name}\"))\n",
        "        \n",
        "    def _load_data(self, dataset):\n",
        "        data_path = f\"data/{dataset}\"\n",
        "        with open(os.path.join(data_path, 'entities.dict')) as fin:\n",
        "            entity2id = dict()\n",
        "            for line in fin:\n",
        "                eid, entity = line.strip().split('\\t')\n",
        "                entity2id[entity] = int(eid)\n",
        "\n",
        "        with open(os.path.join(data_path, 'relations.dict')) as fin:\n",
        "            relation2id = dict()\n",
        "            for line in fin:\n",
        "                rid, relation = line.strip().split('\\t')\n",
        "                relation2id[relation] = int(rid)\n",
        "\n",
        "        self.n_ent = len(entity2id)\n",
        "        self.n_rel = len(relation2id)\n",
        "\n",
        "        self.train_triplets = read_triple(os.path.join(data_path, 'train.txt'), entity2id, relation2id)\n",
        "        self.valid_triplets = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)\n",
        "        self.test_triplets = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)\n",
        "        self.all_true_triplets = self.train_triplets + self.valid_triplets + self.test_triplets\n",
        "\n",
        "        self.facts = torch.Tensor(self.train_triplets).to(torch.long).to(self.device)\n",
        "\n",
        "        train_dataloader_head = DataLoader(\n",
        "            TrainDataset(self.train_triplets, self.n_ent, self.n_rel, self.negative_sample_size, 'head-batch'), \n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True, \n",
        "            collate_fn=TrainDataset.collate_fn\n",
        "        )\n",
        "        train_dataloader_tail = DataLoader(\n",
        "            TrainDataset(self.train_triplets, self.n_ent, self.n_rel, self.negative_sample_size, 'tail-batch'), \n",
        "            batch_size=self.batch_size,\n",
        "            shuffle=True, \n",
        "            collate_fn=TrainDataset.collate_fn\n",
        "        )\n",
        "        self.train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
        "    \n",
        "    def train_one_step(self):\n",
        "        self.model.train() \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        positive_sample, negative_sample, subsampling_weight, mode = next(self.train_iterator)\n",
        "        positive_sample = positive_sample.to(self.device)\n",
        "        negative_sample = negative_sample.to(self.device)\n",
        "        subsampling_weight = subsampling_weight.to(self.device)\n",
        "        \n",
        "        negative_score = self.model((positive_sample, negative_sample), self.facts, mode=mode)\n",
        "        negative_score = (F.softmax(negative_score * self.adversarial_temperature, dim = 1).detach() \n",
        "                              * F.logsigmoid(-negative_score)).sum(dim = 1)\n",
        "        \n",
        "        positive_score = self.model(positive_sample, self.facts)\n",
        "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
        "        \n",
        "        # non uniform weights\n",
        "        positive_loss = -(subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
        "        negative_loss = -(subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
        "        \n",
        "        loss = (positive_loss + negative_loss) / 2 \n",
        "        self.model.regularization() # not implemented yet\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return positive_loss, negative_loss, loss\n",
        "    \n",
        "    def test(self):\n",
        "        self.model.eval() \n",
        "\n",
        "        test_dataloader_head = DataLoader(\n",
        "            TestDataset(\n",
        "                self.test_triplets,\n",
        "                self.all_true_triplets, \n",
        "                self.n_ent, \n",
        "                self.n_rel,\n",
        "                'head-batch'\n",
        "            ),\n",
        "            batch_size=self.test_batch_size,\n",
        "            num_workers=1,\n",
        "            collate_fn=TestDataset.collate_fn\n",
        "        )\n",
        "\n",
        "\n",
        "        test_dataloader_tail = DataLoader(\n",
        "            TestDataset(\n",
        "                self.test_triplets,\n",
        "                self.all_true_triplets, \n",
        "                self.n_ent, \n",
        "                self.n_rel,\n",
        "                'tail-batch'\n",
        "            ),\n",
        "            batch_size=self.test_batch_size,\n",
        "            num_workers=1,\n",
        "            collate_fn=TestDataset.collate_fn\n",
        "        )\n",
        "\n",
        "        test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
        "        logs = [] \n",
        "        step = 0 \n",
        "        total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
        "\n",
        "        print(test_dataset_list)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_dataset in test_dataset_list:\n",
        "                for positive_sample, negative_sample, filter_bias, mode in test_dataset:\n",
        "                    # if step > 10000:\n",
        "                    #     break\n",
        "                    step += 1\n",
        "                    positive_sample = positive_sample.cuda()\n",
        "                    negative_sample = negative_sample.cuda()\n",
        "                    filter_bias = filter_bias.cuda() \n",
        "\n",
        "                    batch_size = positive_sample.size(0)\n",
        "\n",
        "                    score = self.model((positive_sample, negative_sample), self.facts, mode)\n",
        "                    score += filter_bias \n",
        "\n",
        "                    argsort = torch.argsort(score, dim=1, descending=True)\n",
        "\n",
        "                    if mode == 'head-batch':\n",
        "                        positive_arg = positive_sample[:, 0]\n",
        "                    elif mode == 'tail-batch':\n",
        "                        positive_arg = positive_sample[:, 2]\n",
        "                    else:\n",
        "                        raise ValueError(f\"mode {mode} is not supported\")\n",
        "                    \n",
        "                    for i in range(batch_size):\n",
        "                        ranking = (argsort[i, :] == positive_arg[i]).nonzero()\n",
        "                        assert ranking.size(0) == 1\n",
        "\n",
        "                        ranking = 1 + ranking.item()\n",
        "                        logs.append({\n",
        "                            'MRR': 1.0/ranking,\n",
        "                            'MR': float(ranking),\n",
        "                            'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
        "                            'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
        "                            'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
        "                        })\n",
        "\n",
        "            metrics = {}\n",
        "            for metric in logs[0].keys():\n",
        "                metrics[metric] = sum([log[metric] for log in logs]) / len(logs)\n",
        "        \n",
        "        print(metrics)\n",
        "        return metrics \n",
        "    \n",
        "    def run(self, max_steps=10000, log_every=100, resume=False):\n",
        "        if resume:\n",
        "            self.load_model()\n",
        "        self.model.train()\n",
        "        # TODO: handle lr dynamically for Adam\n",
        "        avg_loss, avg_pos_loss, avg_neg_loss = 0, 0, 0\n",
        "        for step in tqdm.tnrange(max_steps):\n",
        "            positive_loss, negative_loss, loss = self.train_one_step()\n",
        "            avg_loss += loss\n",
        "            avg_pos_loss += positive_loss\n",
        "            avg_neg_loss += negative_loss\n",
        "            \n",
        "            if step % log_every == 0:\n",
        "                print(f'Step: {step}')\n",
        "                print(f'Positive Loss: {avg_pos_loss / log_every} ')\n",
        "                print(f'Negative Loss: {avg_neg_loss / log_every}')\n",
        "                print(f'Loss:        : {avg_loss / log_every}')\n",
        "                avg_loss, avg_pos_loss, avg_neg_loss = 0, 0, 0\n",
        "\n",
        "                self.save_model()\n",
        "        self.save_model()\n",
        "            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lra8xIcj3qL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = RotAttTrainer(name=\"10k_1000_dim\", batch_size=5000, checkpoint_dir=\"/content/drive/My Drive/Relation Prediction Train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edclTs1Yk-jD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e8e659b731eb476caf34a9af0a866a93",
            "a2c995dd0be34d5483a35c070dca8184",
            "080dec54dbf0417f93071e6c68ebfc5f",
            "1569f175025543c0b0cd9a518ea68b5b",
            "43639be9f06c4ec28ec4a080cda53c81",
            "4e47c290c33340b08f0757f227a8bbab",
            "84d26dd1b66448d1aa309f2ac6d94717",
            "5c06457741374abd8d81b89044a09560"
          ]
        },
        "outputId": "9a883441-2be1-4ad9-df83-549b44f65e07"
      },
      "source": [
        "%xmode Plain\n",
        "%pdb on\n",
        "trainer.run(5000, 1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Plain\n",
            "Automatic pdb calling has been turned ON\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:187: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8e659b731eb476caf34a9af0a866a93",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=5000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
            "  File \u001b[1;32m\"<ipython-input-10-acc79f21e8ac>\"\u001b[0m, line \u001b[1;32m3\u001b[0m, in \u001b[1;35m<module>\u001b[0m\n    trainer.run(5000, 1000)\n",
            "  File \u001b[1;32m\"<ipython-input-8-229364e71184>\"\u001b[0m, line \u001b[1;32m188\u001b[0m, in \u001b[1;35mrun\u001b[0m\n    positive_loss, negative_loss, loss = self.train_one_step()\n",
            "  File \u001b[1;32m\"<ipython-input-8-229364e71184>\"\u001b[0m, line \u001b[1;32m95\u001b[0m, in \u001b[1;35mtrain_one_step\u001b[0m\n    loss.backward()\n",
            "  File \u001b[1;32m\"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\"\u001b[0m, line \u001b[1;32m198\u001b[0m, in \u001b[1;35mbackward\u001b[0m\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "\u001b[0;36m  File \u001b[0;32m\"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\"\u001b[0;36m, line \u001b[0;32m100\u001b[0;36m, in \u001b[0;35mbackward\u001b[0;36m\u001b[0m\n\u001b[0;31m    allow_unreachable=True)  # allow_unreachable flag\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m\u001b[0;31m:\u001b[0m CUDA out of memory. Tried to allocate 520.00 MiB (GPU 0; 11.17 GiB total capacity; 10.27 GiB already allocated; 158.81 MiB free; 10.71 GiB reserved in total by PyTorch) (malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:289)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x46 (0x7fc610b69536 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1cf1e (0x7fc610db2f1e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1df9e (0x7fc610db3f9e in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10_cuda.so)\nframe #3: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x135 (0x7fc61395ffd5 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xf9310b (0x7fc611f5810b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xfdc9f7 (0x7fc611fa19f7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0x1075389 (0x7fc6498ec389 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #7: <unknown function> + 0x10756c7 (0x7fc6498ec6c7 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0xe3c42e (0x7fc6496b342e in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::TensorIterator::fast_set_up() + 0x5cf (0x7fc6496b42af in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::TensorIterator::build() + 0x4c (0x7fc6496b4b6c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #11: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x146 (0x7fc6496b5216 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #12: at::native::add(at::Tensor const&, at::Tensor const&, c10::Scalar) + 0x45 (0x7fc6493d40a5 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #13: <unknown function> + 0xf8d705 (0x7fc611f52705 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cuda.so)\nframe #14: <unknown function> + 0x10c599b (0x7fc64993c99b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2c0c428 (0x7fc64b483428 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0x10c599b (0x7fc64993c99b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #17: <unknown function> + 0x2d994ad (0x7fc64b6104ad in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #18: <unknown function> + 0x2d9a954 (0x7fc64b611954 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #19: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&) + 0xf5b (0x7fc64b5fd76b in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&, bool) + 0x3d2 (0x7fc64b5fece2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #21: torch::autograd::Engine::thread_init(int) + 0x39 (0x7fc64b5f7359 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::python::PythonEngine::thread_init(int) + 0x38 (0x7fc657d36378 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #23: <unknown function> + 0xbd6df (0x7fc67c1ba6df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #24: <unknown function> + 0x76db (0x7fc67d29c6db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #25: clone + 0x3f (0x7fc67d5d588f in /lib/x86_64-linux-gnu/libc.so.6)\n\n"
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m(100)\u001b[0;36mbackward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     98 \u001b[0;31m    Variable._execution_engine.run_backward(\n",
            "\u001b[0m\u001b[0;32m     99 \u001b[0;31m        \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m--> 100 \u001b[0;31m        allow_unreachable=True)  # allow_unreachable flag\n",
            "\u001b[0m\u001b[0;32m    101 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    102 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> !nvidia-smi\n",
            "*** NameError: name 'nvidia' is not defined\n",
            "ipdb> l\n",
            "\u001b[1;32m     95 \u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     96 \u001b[0m        \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     97 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m     98 \u001b[0m    Variable._execution_engine.run_backward(\n",
            "\u001b[1;32m     99 \u001b[0m        \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m--> 100 \u001b[0;31m        allow_unreachable=True)  # allow_unreachable flag\n",
            "\u001b[0m\u001b[1;32m    101 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    102 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m    103 \u001b[0mdef grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False,\n",
            "\u001b[1;32m    104 \u001b[0m         only_inputs=True, allow_unused=False):\n",
            "\u001b[1;32m    105 \u001b[0m    r\"\"\"Computes and returns the sum of gradients of outputs w.r.t. the inputs.\n",
            "\n",
            "ipdb> u\n",
            "> \u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m(198)\u001b[0;36mbackward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m    196 \u001b[0;31m                \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    197 \u001b[0;31m        \"\"\"\n",
            "\u001b[0m\u001b[0;32m--> 198 \u001b[0;31m        \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    199 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m    200 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m<ipython-input-8-229364e71184>\u001b[0m(95)\u001b[0;36mtrain_one_step\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     93 \u001b[0;31m        \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpositive_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnegative_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     94 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# not implemented yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m        \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     96 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     97 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb0DQbcXTiwM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%xmode Plain \n",
        "%pdb on \n",
        "torch.cuda.empty_cache()\n",
        "trainer.test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GW_e5JYEgoz-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}