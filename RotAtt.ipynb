{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD \n",
    "\n",
    "from torch_scatter import scatter \n",
    "from torchkge.utils.datasets import load_fb15k237\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "import os \n",
    "\n",
    "from layers import * \n",
    "from loss import * \n",
    "from evaluation import * \n",
    "from utils import * \n",
    "from dataloader import * \n",
    "from rotate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_sample_size = 10\n",
    "batch_size = 1000\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/FB15k-237\"\n",
    "with open(os.path.join(data_path, 'entities.dict')) as fin:\n",
    "    entity2id = dict()\n",
    "    for line in fin:\n",
    "        eid, entity = line.strip().split('\\t')\n",
    "        entity2id[entity] = int(eid)\n",
    "\n",
    "with open(os.path.join(data_path, 'relations.dict')) as fin:\n",
    "    relation2id = dict()\n",
    "    for line in fin:\n",
    "        rid, relation = line.strip().split('\\t')\n",
    "        relation2id[relation] = int(rid)\n",
    "\n",
    "n_ent = len(entity2id)\n",
    "n_rel = len(relation2id)\n",
    "\n",
    "train_triplets = read_triple(os.path.join(data_path, 'train.txt'), entity2id, relation2id)\n",
    "valid_triplets = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)\n",
    "test_triplets = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)\n",
    "\n",
    "all_true_triplets = train_triplets + valid_triplets + test_triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_head = DataLoader(\n",
    "    TrainDataset(train_triplets, n_ent, n_rel, negative_sample_size, 'head-batch'), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    collate_fn=TrainDataset.collate_fn\n",
    ")\n",
    "\n",
    "train_dataloader_tail = DataLoader(\n",
    "    TrainDataset(train_triplets, n_ent, n_rel, negative_sample_size, 'tail-batch'), \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True, \n",
    "    collate_fn=TrainDataset.collate_fn\n",
    ")\n",
    "\n",
    "train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer to work with RotAtt\n",
    "class RotAttTrainer:\n",
    "    def __init__(self, name, model: nn.Module, dataset=\"FB15k-237\", n_epochs=1000, batch_size=2000, device=\"cuda\", \n",
    "        optim_ = \"sgd\", lr = 0.001, checkpoint_dir=\"checkpoints\"):\n",
    "        self.name = name\n",
    "        \n",
    "        self.work_threads = 4 \n",
    "        self.lr = lr \n",
    "        self.weight_decay = None\n",
    "        self.n_epochs = n_epochs\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr)\n",
    "        self.adversarial_temperature = 1.0\n",
    "        self.negative_sample_size = 10\n",
    "    \n",
    "    \n",
    "        self._load_data(dataset)\n",
    "        train_dataloader_head = DataLoader(\n",
    "            TrainDataset(self.train_triplets, self.n_ent, self.n_rel, self.negative_sample_size, 'head-batch'), \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True, \n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "        train_dataloader_tail = DataLoader(\n",
    "            TrainDataset(self.train_triplets, self.n_ent, self.n_rel, self.negative_sample_size, 'tail-batch'), \n",
    "            batch_size=batch_size,\n",
    "            shuffle=True, \n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "        self.train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
    "        \n",
    "    def _load_data(self, name):\n",
    "        data_path = f\"data/{name}\"\n",
    "        with open(os.path.join(data_path, 'entities.dict')) as fin:\n",
    "            entity2id = dict()\n",
    "            for line in fin:\n",
    "                eid, entity = line.strip().split('\\t')\n",
    "                entity2id[entity] = int(eid)\n",
    "\n",
    "        with open(os.path.join(data_path, 'relations.dict')) as fin:\n",
    "            relation2id = dict()\n",
    "            for line in fin:\n",
    "                rid, relation = line.strip().split('\\t')\n",
    "                relation2id[relation] = int(rid)\n",
    "\n",
    "        self.n_ent = len(entity2id)\n",
    "        self.n_rel = len(relation2id)\n",
    "\n",
    "        self.train_triplets = read_triple(os.path.join(data_path, 'train.txt'), entity2id, relation2id)\n",
    "        self.valid_triplets = read_triple(os.path.join(data_path, 'valid.txt'), entity2id, relation2id)\n",
    "        self.test_triplets = read_triple(os.path.join(data_path, 'test.txt'), entity2id, relation2id)\n",
    "\n",
    "        all_true_triplets = train_triplets + valid_triplets + test_triplets\n",
    "\n",
    "    \n",
    "    def train_one_step(self):\n",
    "        self.model.train() \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        positive_sample, negative_sample, subsampling_weight, mode = next(self.train_iterator)\n",
    "        positive_sample.to(self.device)\n",
    "        negative_sample.to(self.device)\n",
    "        subsampling_weight.to(self.device)\n",
    "        \n",
    "        triplets = positive_sample.repeat((self.negative_sample_size + 1, 1))\n",
    "        triplets[self.batch_size:, 2] = negative_sample.flatten()\n",
    "        \n",
    "        \n",
    "        negative_score = model(triplets, mode=mode)\n",
    "        negative_score = (F.softmax(negative_score * self.adversarial_temperature, dim = 1).detach() \n",
    "                              * F.logsigmoid(-negative_score)).sum(dim = 1)\n",
    "        \n",
    "        positive_score = model(positive_sample)\n",
    "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
    "        \n",
    "        # non uniform weights\n",
    "        positive_loss = -(subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
    "        negative_loss = -(subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
    "        \n",
    "        loss = (positive_loss + negative_loss) / 2 \n",
    "        self.model.regularization() # not implemented yet\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return positive_loss, negative_loss, loss\n",
    "        \n",
    "    \n",
    "    def run(self, max_steps=10000, log_every=1000):\n",
    "        self.model.train()\n",
    "        # TODO: handle lr dynamically for Adam\n",
    "        avg_loss, avg_pos_loss, avg_neg_loss = 0, 0, 0\n",
    "        for step in range(max_steps):\n",
    "            positive_loss, negative_loss, loss = self.train_one_step()\n",
    "            avg_loss += loss\n",
    "            avg_pos_loss += positive_loss\n",
    "            avg_neg_loss += negative_loss\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                print(f'Step: {step}')\n",
    "                print(f'Positive Loss: {avg_pos_loss}')\n",
    "                print(f'Negative Loss: {avg_neg_loss}')\n",
    "                print(f'Loss:        : {avg_loss}')\n",
    "                avg_loss, avg_pos_loss, avg_neg_loss = 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotAttLayer(nn.Module):\n",
    "    def __init__(self, n_entities, n_relations, in_dim, out_dim, input_drop=0.5, \n",
    "                 margin=6.0, epsilon=2.0, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.margin = margin \n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.a = nn.Linear(3 * in_dim, out_dim).to(device)\n",
    "        nn.init.xavier_normal_(self.a.weight.data, gain=1.414)\n",
    "\n",
    "        self.a_2 = nn.Linear(out_dim, 1).to(device)\n",
    "        nn.init.xavier_normal_(self.a_2.weight.data, gain=1.414)\n",
    "\n",
    "        self.sparse_neighborhood_aggregation = SparseNeighborhoodAggregation()\n",
    "        \n",
    "        self.ent_embed_range = nn.Parameter(\n",
    "            torch.Tensor([(self.margin + self.epsilon) / self.out_dim]), \n",
    "            requires_grad = False\n",
    "        )\n",
    "        \n",
    "        self.rel_embed_range = nn.Parameter(\n",
    "            torch.Tensor([(self.margin + self.epsilon) / self.out_dim]),\n",
    "            requires_grad = False\n",
    "        )\n",
    "\n",
    "        self.ent_embed = nn.Embedding(n_entities, in_dim, max_norm=1, norm_type=2).to(device)\n",
    "        self.rel_embed = nn.Embedding(n_relations, in_dim, max_norm=1, norm_type=2).to(device)\n",
    "        \n",
    "        nn.init.uniform_(self.ent_embed.weight.data, -self.ent_embed_range.item(), self.ent_embed_range.item())\n",
    "        nn.init.uniform_(self.rel_embed.weight.data, -self.rel_embed_range.item(), self.rel_embed_range.item())\n",
    "\n",
    "        self.input_drop = nn.Dropout(input_drop)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(3 * in_dim).to(device)\n",
    "        self.bn1 = nn.BatchNorm1d(out_dim).to(device)\n",
    "        \n",
    "    def forward(self, triplets, eval=False, mode=\"head\"):\n",
    "\n",
    "        N = self.n_entities\n",
    "        n = len(triplets)\n",
    "\n",
    "        h = torch.cat((\n",
    "            self.ent_embed(triplets[:, 0]),\n",
    "            self.ent_embed(triplets[:, 2]),\n",
    "            self.rel_embed(triplets[:, 1])\n",
    "        ), dim=1)\n",
    "        h_ = torch.cat((\n",
    "            self.ent_embed(triplets[:, 2]),\n",
    "            self.ent_embed(triplets[:, 0]),\n",
    "            -self.rel_embed(triplets[:, 1])\n",
    "        ), dim=1)\n",
    "\n",
    "        h = torch.cat((h, h_))\n",
    "\n",
    "        h = self.input_drop(self.bn0(h))\n",
    "        c = self.bn1(self.a(h))\n",
    "        b = -F.leaky_relu(self.a_2(c))\n",
    "        e_b = torch.exp(b)\n",
    "\n",
    "        temp = triplets.t()\n",
    "        edges = torch.stack((\n",
    "            torch.cat([temp[0], temp[2]]),\n",
    "            torch.cat([temp[2], temp[0]])\n",
    "        ))\n",
    "\n",
    "        ebs = self.sparse_neighborhood_aggregation(edges, e_b, N, e_b.shape[0], 1)\n",
    "        temp1 = e_b * c\n",
    "\n",
    "        hs = self.sparse_neighborhood_aggregation(edges, temp1,  N, e_b.shape[0], self.out_dim)\n",
    "\n",
    "        ebs[ebs == 0] = 1e-12\n",
    "        h_ent = hs / ebs\n",
    "\n",
    "        # do I need this?\n",
    "        index = triplets[:, 2]\n",
    "        h_rel  = scatter(temp1[ : temp1.shape[0]//2, :], index=index, dim=0, reduce=\"mean\") \n",
    "        # h_rel_ =  scatter(temp1[temp1.shape[0]//2 : , :], index=index, dim=0, reduce=\"mean\")\n",
    "\n",
    "        return F.elu(h_ent), F.elu(h_rel)\n",
    "    \n",
    "class RotAtt(nn.Module):\n",
    "    def __init__(self, n_ent, n_rel, in_dim, out_dim, n_heads=1, input_drop=0.5, negative_rate = 10, margin=6.0, epsilon=2.0, device=\"cuda\"):\n",
    "        super().__init__() \n",
    "\n",
    "        self.n_heads = n_heads \n",
    "        self.device = device\n",
    "\n",
    "        self.in_dim = in_dim \n",
    "        self.out_dim = out_dim \n",
    "        self.margin = margin\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.a = nn.ModuleList([\n",
    "            RotAttLayer(\n",
    "                n_ent, n_rel, in_dim, out_dim, input_drop, margin=margin, epsilon=epsilon\n",
    "            )\n",
    "        for _ in range(self.n_heads)])\n",
    "\n",
    "        self.ent_transform = nn.Linear(n_heads * out_dim, out_dim).to(device)\n",
    "        self.rel_transform = nn.Linear(n_heads * out_dim, out_dim // 2).to(device)\n",
    "\n",
    "        \n",
    "        self.ent_embed_range = nn.Parameter(\n",
    "            torch.Tensor([(self.margin + self.epsilon) / self.out_dim]), \n",
    "            requires_grad = False\n",
    "        )\n",
    "        \n",
    "        self.rel_embed_range = nn.Parameter(\n",
    "            torch.Tensor([(self.margin + self.epsilon) / self.out_dim]),\n",
    "            requires_grad = False\n",
    "        )\n",
    "\n",
    "        self.pi = nn.Parameter(torch.Tensor([3.14159265358979323846])).to(device)\n",
    "        self.pi.detach()\n",
    "\n",
    "        self.negative_rate = negative_rate\n",
    "    \n",
    "    def rotate(self, h, r, t, mode):\n",
    "        pi = 3.14159265358979323846\n",
    "\n",
    "        re_head, im_head = torch.chunk(h, 2, dim=-1)\n",
    "        re_tail, im_tail = torch.chunk(t, 2, dim=-1)\n",
    "\n",
    "        phase_relation = r / (self.rel_embed_range.item() / pi) # phases of rel distributed uniformly in [-pi, pi]\n",
    "        \n",
    "        re_relation = torch.cos(phase_relation)\n",
    "        im_relation = torch.sin(phase_relation)\n",
    "        \n",
    "        if mode == 'head-batch':\n",
    "            re_score = re_relation * re_tail + im_relation * im_tail\n",
    "            im_score = re_relation * im_tail - im_relation * re_tail\n",
    "            re_score = re_score - re_head\n",
    "            im_score = im_score - im_head\n",
    "        else:\n",
    "            re_score = re_head * re_relation - im_head * im_relation\n",
    "            im_score = re_head * im_relation + im_head * re_relation\n",
    "            re_score = re_score - re_tail\n",
    "            im_score = im_score - im_tail\n",
    "\n",
    "        score = torch.stack([re_score, im_score], dim = 0)\n",
    "        score = score.norm(dim = 0)\n",
    "\n",
    "        score = self.gamma.item() - score.sum(dim = 2)\n",
    "        return score\n",
    "\n",
    "    def forward(self, triplets, mode=\"single\"):\n",
    "        n = len(triplets)\n",
    "        batch_size = len(triplets) / self.negative_rate\n",
    "        out = [a(triplets) for a in self.a]\n",
    "        ent_embed = self.ent_transform(torch.cat([o[0] for o in out], dim=1))\n",
    "        rel_embed = self.rel_transform(torch.cat([o[1] for o in out], dim=1))\n",
    "        \n",
    "        if mode == 'tail-batch':\n",
    "            head_part = triplets[:self.negative_rate, ]\n",
    "            tail_part = triplets[self.negative_rate:, 2].view(-1, self.negative_rate)\n",
    "            \n",
    "            head = ent_embed[head_part[:, 0]].unsqueeze(1)\n",
    "            tail = ent_embed[tail_part.view(-1)].view(batch, self.negative_rate, -1)\n",
    "            rel  = rel_embed[head_part[:, 1]]\n",
    "        elif mode == 'head-batch':\n",
    "            head_part = triplets[self.negative_rate:, 2].view(-1, self.negative_rate)\n",
    "            tail_part = triplets[:self.negative_rate, ]\n",
    "            \n",
    "            head = ent_embed[head_part.view(-1)].view(batch, self.negative_rate, -1)\n",
    "            tail = ent_embed[tail_part[:, 2]].unsqueeze(1)\n",
    "            rel  = rel_embed[tail_part[:, 1]].unsqueeze(1)\n",
    "        elif mode == 'single':\n",
    "            head = ent_embed[triplets[:, 0]].unsqueeze(1)\n",
    "            rel  = rel_embed[triplets[:, 1]].unsqueeze(1)\n",
    "            tail = ent_embed[triplets[:, 2]].unsqueeze(1)\n",
    "            \n",
    "        score = self.rotate(head, rel, tail, mode)\n",
    "\n",
    "    def predict(self, data, mode=\"tail_batch\"):\n",
    "        score = -self.forward(data, mode, eval=True)\n",
    "        return score.cpu().data.numpy()\n",
    "    \n",
    "    def regularization(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
